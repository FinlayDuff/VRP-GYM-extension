{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "%%capture \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "!pip install -e ..\n",
    "\n",
    "\n",
    "from gym_vrp.envs.vrp import VRPEnv, VRPVariant\n",
    "from gym_vrp.graph.graph import VRPGraph\n",
    "\n",
    "env = VRPEnv.create(variant=VRPVariant.DEFAULT_VRP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "graph = VRPGraph(10, 5)\n",
    "\n",
    "graph.color_edge(0, 1)\n",
    "graph.color_edge(1, 2)\n",
    "graph.color_edge(2, 0)\n",
    "\n",
    "ax = plt.subplot(1,1,1)\n",
    "graph.draw(ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from gym_vrp.envs.vrp import DefaultVRPEnv\n",
    "import numpy as np\n",
    "from IPython import display\n",
    "\n",
    "actions = np.array([1]*2)[:, None]\n",
    "network = DefaultVRPEnv(num_nodes=10, batch_size=2, num_draw=2)\n",
    "network.step(actions)\n",
    "print(network.get_state())\n",
    "img = network.render()\n",
    "# display.clear_output(wait=True)\n",
    "actions = np.array([2]*2)[:, None]\n",
    "network.step(actions)\n",
    "img = network.render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
    "\n",
    "env = DefaultVRPEnv(num_nodes=10, batch_size=12, num_draw=9)\n",
    "vid = VideoRecorder(env, \"../videos/test.mp4\")\n",
    "vid.frames_per_sec = 1\n",
    "\n",
    "\n",
    "env.reset()\n",
    "actions = np.asarray([1]*12)[:, None]\n",
    "env.step(actions)\n",
    "env.render()\n",
    "vid.capture_frame()\n",
    "actions = np.asarray([2]*12)[:, None]\n",
    "env.step(actions)\n",
    "env.render()\n",
    "vid.capture_frame()\n",
    "actions = np.asarray([3]*12)[:, None]\n",
    "env.step(actions)\n",
    "env.render()\n",
    "vid.capture_frame()\n",
    "\n",
    "vid.close()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "!pip install -e .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4001,  0.3419,  0.1960,  ...,  0.2589, -0.5468, -0.2861],\n",
       "         [-0.0232,  0.3071,  0.3390,  ...,  0.1616, -1.1264, -0.8162],\n",
       "         [ 0.0881,  0.2974,  0.3816,  ...,  0.1331, -1.2965, -0.9720],\n",
       "         ...,\n",
       "         [ 0.0664,  0.2064,  0.3032,  ...,  0.0897, -1.4515, -1.0638],\n",
       "         [ 0.0214,  0.3674,  0.4044,  ...,  0.1840, -1.0645, -0.7942],\n",
       "         [-0.4661,  0.0608, -0.0455,  ...,  0.1245, -1.0270, -0.5710]],\n",
       "\n",
       "        [[-0.4625,  0.1343,  0.0115,  ...,  0.1625, -0.8829, -0.4789],\n",
       "         [-0.2270,  0.2052,  0.1706,  ...,  0.1505, -1.0575, -0.6883],\n",
       "         [-0.2732,  0.2474,  0.1817,  ...,  0.1825, -0.9095, -0.5734],\n",
       "         ...,\n",
       "         [-0.3090,  0.1758,  0.1116,  ...,  0.1522, -1.0063, -0.6216],\n",
       "         [-0.1995,  0.2420,  0.2107,  ...,  0.1642, -1.0201, -0.6752],\n",
       "         [-0.0084,  0.4265,  0.4356,  ...,  0.2215, -0.9045, -0.6782]],\n",
       "\n",
       "        [[-0.2286,  0.2921,  0.2355,  ...,  0.1967, -0.8793, -0.5720],\n",
       "         [-0.0806,  0.3235,  0.3255,  ...,  0.1823, -1.0155, -0.7208],\n",
       "         [ 0.0516,  0.2763,  0.3493,  ...,  0.1297, -1.2898, -0.9528],\n",
       "         ...,\n",
       "         [-0.2724,  0.3168,  0.2344,  ...,  0.2189, -0.7700, -0.4832],\n",
       "         [-0.0383,  0.3648,  0.3757,  ...,  0.1952, -0.9890, -0.7208],\n",
       "         [-0.1499,  0.3586,  0.3209,  ...,  0.2153, -0.8508, -0.5856]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.4204,  0.0860, -0.0060,  ...,  0.1282, -1.0378, -0.5966],\n",
       "         [ 0.0071,  0.1796,  0.2565,  ...,  0.0880, -1.4256, -1.0229],\n",
       "         [-0.4005,  0.0672, -0.0113,  ...,  0.1141, -1.1027, -0.6469],\n",
       "         ...,\n",
       "         [ 0.0089,  0.4371,  0.4514,  ...,  0.2234, -0.9063, -0.6864],\n",
       "         [ 0.1049,  0.4446,  0.5002,  ...,  0.2073, -1.0209, -0.8000],\n",
       "         [-0.1517,  0.3313,  0.2995,  ...,  0.2012, -0.9039, -0.6193]],\n",
       "\n",
       "        [[ 0.0567,  0.1933,  0.2890,  ...,  0.0848, -1.4649, -1.0686],\n",
       "         [-0.0051,  0.2516,  0.3053,  ...,  0.1285, -1.2632, -0.9125],\n",
       "         [-0.1748,  0.3937,  0.3362,  ...,  0.2390, -0.7460, -0.5074],\n",
       "         ...,\n",
       "         [-0.0616,  0.2603,  0.2864,  ...,  0.1449, -1.1693, -0.8284],\n",
       "         [-0.0116,  0.2779,  0.3222,  ...,  0.1437, -1.2012, -0.8696],\n",
       "         [-0.1199,  0.1868,  0.2048,  ...,  0.1184, -1.2396, -0.8503]],\n",
       "\n",
       "        [[-0.3430,  0.1837,  0.1023,  ...,  0.1635, -0.9444, -0.5676],\n",
       "         [-0.4325,  0.1254,  0.0182,  ...,  0.1515, -0.9415, -0.5292],\n",
       "         [-0.3734,  0.2717,  0.1550,  ...,  0.2163, -0.7249, -0.4127],\n",
       "         ...,\n",
       "         [-0.2980,  0.1844,  0.1231,  ...,  0.1544, -1.0038, -0.6245],\n",
       "         [-0.0486,  0.2911,  0.3155,  ...,  0.1584, -1.1246, -0.8047],\n",
       "         [-0.1408,  0.3253,  0.2999,  ...,  0.1958, -0.9305, -0.6411]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from agents.graph_encoder import GraphEncoder\n",
    "from gym_vrp.envs.vrp import DefaultVRPEnv\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "state = torch.from_numpy(env.reset()).float()\n",
    "depot_idx = np.argmax(state[:,:, 2], axis=1)\n",
    "\n",
    "encoder = GraphEncoder(128, 512, 2, 8, 3, 2)\n",
    "encoder(state[:,:,:2], depot_idx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.43214805 0.75213452 0.         0.        ]\n",
      " [0.82960225 0.37903377 1.         0.        ]\n",
      " [0.09654961 0.25613958 0.         0.        ]\n",
      " [0.59193516 0.4764772  0.         0.        ]\n",
      " [0.48793457 0.45851455 0.         0.        ]\n",
      " [0.52459262 0.44201532 0.         0.        ]\n",
      " [0.85263492 0.43343897 0.         0.        ]\n",
      " [0.82687037 0.50934207 0.         0.        ]\n",
      " [0.08637698 0.66003986 0.         0.        ]\n",
      " [0.20659461 0.84727532 0.         0.        ]]\n",
      "[[0.88979366 0.76456197 0.         0.        ]\n",
      " [0.69824848 0.33549817 0.         0.        ]\n",
      " [0.14768558 0.062636   0.         0.        ]\n",
      " [0.2419017  0.43228148 0.         0.        ]\n",
      " [0.52199627 0.77308355 0.         0.        ]\n",
      " [0.95874092 0.11732048 0.         0.        ]\n",
      " [0.10700414 0.58969472 0.         0.        ]\n",
      " [0.74539807 0.84815038 1.         0.        ]\n",
      " [0.93583208 0.98342624 0.         0.        ]\n",
      " [0.39980169 0.38033518 0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "from gym_vrp.envs.vrp import DefaultVRPEnv\n",
    "env = DefaultVRPEnv(num_nodes=10, batch_size=12, num_draw=9, seed=0)\n",
    "env_two = DefaultVRPEnv(num_nodes=10, batch_size=12, num_draw=9, seed=0)\n",
    "\n",
    "state=env.reset()\n",
    "state_two = env_two.reset()\n",
    "\n",
    "state_two = env_two.get_state()\n",
    "\n",
    "print(state[0])\n",
    "print(state_two[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from gym_vrp.envs.vrp import DefaultVRPEnv\n",
    "from agents.graph_agent import VRPAgent\n",
    "\n",
    "env = DefaultVRPEnv(num_nodes=10, batch_size=12, num_draw=9, seed=0)\n",
    "\n",
    "agent = VRPAgent(depot_dim=2, node_dim=2)\n",
    "\n",
    "agent.train(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "c = torch.Tensor([1, 1, 2])\n",
    "d = torch.Tensor([1, 1, 2])\n",
    "print(torch.sub(c, d))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916353694c05b0bed1fe8308b7e69d5871a584faf597572c5b22618c7037ecff"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('gym_vrp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
